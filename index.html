<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="Learning to Generate Human-Human-Object Interactions from Textual Descriptions">
  <meta property="og:title" content="HHOI"/>
  <meta property="og:description" content="Project page for Learning to Generate Human-Human-Object Interactions from Textual Descriptions"/>
  <meta property="og:url" content="https://tlb_miss.github.io/hhoi" />
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/images/teaser.png" />
  <meta property="og:image:width" content="2986"/>
  <meta property="og:image:height" content="1068"/>


  <meta name="twitter:title" content="HHOI">
  <meta name="twitter:description" content="Learning to Generate Human-Human-Object Interactions from Textual Descriptions">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/teaser.png">
  <meta name="twitter:card" content="HHOI">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="hhoi">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Learning to Generate Human-Human-Object Interactions from Textual Descriptions</title>
  <link rel="icon" type="image/x-icon" href="static/images/logo.png">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  integrity="sha384-KCoi1hWvqIEzUpQkr0/eqz25VwvpkQ+gSTsaZfGUO+S4zESTXZuAZS86EANASVca" crossorigin="anonymous"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css"
  integrity="sha384-cxs0Bj979VdNByRZzAJ2KJWG8vH/kG7fwWZAIZW3Tmsw0rAuP/pu3I3LitFWEs18" crossorigin="anonymous">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"
  integrity="sha384-ZvpUoO/+PpLXR1lu4jmpXWu80pZlYUAfxl5NsBMWOEPSjUn/6Z/hRTt8+pR6L4N2"
  crossorigin="anonymous"></script>
<script src="https://documentcloud.adobe.com/view-sdk/main.js"
  integrity="sha384-/r+//Gr1EDmAtu40BA3mLUNz1fa0zSRzML/uhwUi7y0V7QulZ9Z8QDvvHJlATHx6"
  crossorigin="anonymous"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>

  <script type="text/javascript" async
  src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script type="text/javascript" async
    id="MathJax-script"
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
  </script>

  <style>
    .video-wrapper {
      position: relative;
      display: inline-block; /* 가운데 정렬용 */
    }
  
    .label-over-video {
      position: absolute;
      top: 8px;               /* 영상 위에서부터 거리 조절 */
      left: 50%;
      transform: translateX(-50%);
      background: rgba(0, 0, 0, 0.6);  /* 살짝 반투명 배경 */
      color: #fff;
      padding: 2px 8px;
      border-radius: 4px;
      font-weight: 600;
      font-size: 0.9rem;
    }
  </style>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <!-- <div class="container is-max-desktop"> -->
      <div class="container">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Learning to Generate Human-Human-Object Interactions from Textual Descriptions</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://nagooon.github.io/" target="_blank">Jeonghyeon Na<sup>*</sup></a>,&nbsp
              </span>
                <span class="author-block">
                  <a href="https://tlb-miss.github.io/" target="_blank">Sangwon Baik<sup>*</sup></a>,&nbsp
                </span>
                <span class="author-block">
                  <a href="https://blog.sulwon.com/" target="_blank">Inhee Lee</a>,&nbsp
                </span>
                <span class="author-block">
                  <a href="https://junc0ng.github.io/" target="_blank">Junyoung Lee</a>,&nbsp
                </span>
                  <span class="author-block">
                    <a href="https://jhugestar.github.io" target="_blank">Hanbyul Joo<sup>†</sup></a>
                  </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">Seoul National University</span>
                    <span class="eql-cntrb"><small><br>* Equal Contribution &nbsp &nbsp † Corresponding Author</small></span>
                  </div>
                  
                  <div class="columns is-centered">
                    <div class="is-size-4 publication-venue">NeurIPS 2025
                    </div>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                      <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/2511.20446" target="_blank"
                          class="external-link button is-normal is-rounded is-dark">
                          <span class="icon">
                            <i class="fas fa-file-pdf"></i>
                          </span>
                          <span>Paper</span>
                        </a>
                      </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/snuvclab/hhoi" target="_blank"
                       class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fab fa-github"></i>
                      </span>
                      <span>Code</span>
                    </a>
                  </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser image-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="static/images/teaser.png" alt="Teaser Image"/>
      <h2 class="subtitle has-text-centered">
        Given a textual description of HHOI (Human–Human–Object Interaction) involving interactions among multiple people and between people and an object, our model generates the body poses of all humans as well as their global orientations and translations with respect to the given object.
      </h2>
    </div>
  </div>
</section>

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            The way humans interact with each other, including interpersonal distances, spatial configuration, and motion, varies significantly across different situations. To enable machines to understand such complex, context-dependent behaviors, it is essential to model multiple people in relation to the surrounding scene context. In this paper, we present a novel research problem to model the correlations between two people engaged in a shared interaction involving an object. We refer to this formulation as Human-Human-Object Interactions (HHOIs). To overcome the lack of dedicated datasets for HHOIs, we present a newly captured HHOIs dataset and a method to synthesize HHOI data by leveraging image generative models. As an intermediary, we obtain individual human-object interaction (HOIs) and human-human interaction (HHIs) from the HHOIs, and with these data, we train an text-to-HOI and text-to-HHI model using score-based diffusion model. Finally, we present a unified generative framework that integrates the two individual model, capable of synthesizing complete HHOIs in a single advanced sampling process. Our method extends HHOI generation to multi-human settings, enabling interactions involving more than two individuals. Experimental results show that our method generates realistic HHOIs conditioned on textual descriptions, outperforming previous approaches that focus only on single-human HOIs. Furthermore, we introduce multi-human motion generation involving objects as an application of our framework.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Method -->
<section class="hero">
  <div class="hero-body">
    <div class="container has-text-centered">
      <h2 class="title is-3">Dataset Capture</h2>
      <img src="static/images/dataset.png" alt="Method Image" style="width: 65%;"/>
      <div style="max-width: 900px; margin: 0 auto; text-align: center; line-height: 1.6;">
        To address the lack of diverse HHOI data, we introduce a newly collected 3D dataset captured using a multi-camera system, specifically designed to support the training and evaluation of HHOI models. The object and human poses are tracked with AruCo markers and DWPose respectively.
      </div>

      <br>
      <br>
      <h2 class="title is-3">Method Overview</h2>
      <img src="static/images/method.png" alt="Method Image" style="width: 65%;"/>
      <div style="max-width: 1000px; margin: 0 auto; text-align: center; line-height: 1.6;">
        We split the captured HHOI dataset into an HOI dataset and an HHI dataset, and train separate HOI and HHI diffusion models on each subset. During inference, by leveraging our proposed advanced HHOI sampling strategy, we jointly sample from both the HOI and HHI diffusion models, enabling generation of interactions involving multiple humans.
      </div>
    </div>
  </div>
</section>

<!-- Video presentation -->
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container has-text-centered">
      <!-- Paper video. -->
      <h2 class="title is-3">Video Presentation</h2>
      <div class="columns is-centered has-text-centered">
        <div class="column is-fullwidth">
          
          <video controls style="width: 100%; max-width: 1600px;">
            <source src="static/videos/hhoi_video.mp4" type="video/mp4">
          </video>
          <!--<div class="publication-video">-->
            <!-- Youtube embed code here -->
            <!--<iframe src="https://www.youtube.com/embed/JkaxUblCGz0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
          </div>-->
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Results -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container has-text-centered">
      <h2 class="title is-3">Results</h2>
      
      <!-- Comparison with Competitors -->
      <h3 class="title is-4" style="margin-top: 50px;">Comparison with Competitors on HHOI Generation</h3>
      <div id="competitor-carousel" class="carousel results-carousel">
        <!-- 1. umbrella -->
        <div class="item">
          <div class="columns is-vcentered is-desktop">
            <!-- Ours -->
            <div class="column has-text-centered">
              <div class="video-wrapper">
                <span class="label-over-video">Ours</span>
                <video controls autoplay loop muted style="max-width: 100%;">
                  <source src="static/videos/umbrella_ours.mp4" type="video/mp4">
                </video>
              </div>
            </div>
            <!-- GenZI -->
            <div class="column has-text-centered">
              <div class="video-wrapper">
                <span class="label-over-video">GenZI</span>
                <video controls autoplay loop muted style="max-width: 100%;">
                  <source src="static/videos/umbrella_GenZI.mp4" type="video/mp4">
                </video>
              </div>
            </div>
            <!-- Depth Opt. -->
            <div class="column has-text-centered">
              <div class="video-wrapper">
                <span class="label-over-video">Depth Opt.</span>
                <video controls autoplay loop muted style="max-width: 100%;">
                  <source src="static/videos/umbrella_depth.mp4" type="video/mp4">
                </video>
              </div>
            </div>
          </div>
          <h2 class="subtitle has-text-centered">
            <i>Two people are sharing an umbrella.</i>
          </h2>
        </div>
        <!-- 2. wheelchair -->
        <div class="item">
          <div class="columns is-vcentered is-desktop">
            <!-- Ours -->
            <div class="column has-text-centered">
              <div class="video-wrapper">
                <span class="label-over-video">Ours</span>
                <video controls autoplay loop muted style="max-width: 100%;">
                  <source src="static/videos/wheelchair_ours.mp4" type="video/mp4">
                </video>
              </div>
            </div>
            <!-- GenZI -->
            <div class="column has-text-centered">
              <div class="video-wrapper">
                <span class="label-over-video">GenZI</span>
                <video controls autoplay loop muted style="max-width: 100%;">
                  <source src="static/videos/wheelchair_GenZI.mp4" type="video/mp4">
                </video>
              </div>
            </div>
            <!-- Depth Opt. -->
            <div class="column has-text-centered">
              <div class="video-wrapper">
                <span class="label-over-video">Depth Opt.</span>
                <video controls autoplay loop muted style="max-width: 100%;">
                  <source src="static/videos/wheelchair_depth.mp4" type="video/mp4">
                </video>
              </div>
            </div>
          </div>
          <h2 class="subtitle has-text-centered">
            <i>Two people are helping another person in a wheelchair.</i>
          </h2>
        </div>
        <!-- 3. campfire -->
        <div class="item">
          <div class="columns is-vcentered is-desktop">
            <!-- Ours -->
            <div class="column has-text-centered">
              <div class="video-wrapper">
                <span class="label-over-video">Ours</span>
                <video controls autoplay loop muted style="max-width: 100%;">
                  <source src="static/videos/campfire_ours.mp4" type="video/mp4">
                </video>
              </div>
            </div>
            <!-- GenZI -->
            <div class="column has-text-centered">
              <div class="video-wrapper">
                <span class="label-over-video">GenZI</span>
                <video controls autoplay loop muted style="max-width: 100%;">
                  <source src="static/videos/campfire_GenZI.mp4" type="video/mp4">
                </video>
              </div>
            </div>
            <!-- Depth Opt. -->
            <div class="column has-text-centered">
              <div class="video-wrapper">
                <span class="label-over-video">Depth Opt.</span>
                <video controls autoplay loop muted style="max-width: 100%;">
                  <source src="static/videos/campfire_depth.mp4" type="video/mp4">
                </video>
              </div>
            </div>
          </div>
          <h2 class="subtitle has-text-centered">
            <i>Three people are sitting next to a campfire.</i>
          </h2>
        </div>
      </div>

      <!-- HHOI Generation with Multiple Humans -->
      <h3 class="title is-4" style="margin-top: 50px;">HHOI Generation with Multiple Humans</h3>
      <div id="multi-carousel" class="carousel results-carousel">
        <!-- 1. laptop -->
        <div class="item">
          <div class="columns is-vcentered is-desktop">
            <!-- N=2 -->
            <div class="column has-text-centered">
              <div class="video-wrapper">
                <span class="label-over-video">N = 2</span>
                <video controls autoplay loop muted style="max-width: 100%;">
                  <source src="static/videos/laptop_2.mp4" type="video/mp4">
                </video>
              </div>
            </div>
            <!-- N=3 -->
            <div class="column has-text-centered">
              <div class="video-wrapper">
                <span class="label-over-video">N = 3</span>
                <video controls autoplay loop muted style="max-width: 100%;">
                  <source src="static/videos/laptop_3.mp4" type="video/mp4">
                </video>
              </div>
            </div>
            <!-- N=4 -->
            <div class="column has-text-centered">
              <div class="video-wrapper">
                <span class="label-over-video">N = 4</span>
                <video controls autoplay loop muted style="max-width: 100%;">
                  <source src="static/videos/laptop_4.mp4" type="video/mp4">
                </video>
              </div>
            </div>
            <!-- N=5 -->
            <div class="column has-text-centered">
              <div class="video-wrapper">
                <span class="label-over-video">N = 5</span>
                <video controls autoplay loop muted style="max-width: 100%;">
                  <source src="static/videos/laptop_5.mp4" type="video/mp4">
                </video>
              </div>
            </div>
          </div>
          <h2 class="subtitle has-text-centered">
            <i>People are looking at a laptop while sitted.</i>
          </h2>
        </div>
        <!-- 2. whiteboard -->
        <div class="item">
          <div class="columns is-vcentered is-desktop">
            <!-- N=2 -->
            <div class="column has-text-centered">
              <div class="video-wrapper">
                <span class="label-over-video">N = 2</span>
                <video controls autoplay loop muted style="max-width: 100%;">
                  <source src="static/videos/wb_2.mp4" type="video/mp4">
                </video>
              </div>
            </div>
            <!-- N=3 -->
            <div class="column has-text-centered">
              <div class="video-wrapper">
                <span class="label-over-video">N = 3</span>
                <video controls autoplay loop muted style="max-width: 100%;">
                  <source src="static/videos/wb_3.mp4" type="video/mp4">
                </video>
              </div>
            </div>
            <!-- N=4 -->
            <div class="column has-text-centered">
              <div class="video-wrapper">
                <span class="label-over-video">N = 4</span>
                <video controls autoplay loop muted style="max-width: 100%;">
                  <source src="static/videos/wb_4.mp4" type="video/mp4">
                </video>
              </div>
            </div>
            <!-- N=5 -->
            <div class="column has-text-centered">
              <div class="video-wrapper">
                <span class="label-over-video">N = 5</span>
                <video controls autoplay loop muted style="max-width: 100%;">
                  <source src="static/videos/wb_5.mp4" type="video/mp4">
                </video>
              </div>
            </div>
          </div>
          <h2 class="subtitle has-text-centered">
            <i>People are having a discussion in front of a whiteboard.</i>
          </h2>
        </div>
      </div>
      
      <!-- Human Motion In-Betweening Section -->
      <h3 class="title is-4" style="margin-top: 100px;">Application: Human Motion In-Betweening</h3>
      <div id="scene-editing-carousel" class="carousel results-carousel">
        <!-- Loop: 각 동영상 항목 -->
        <div class="item">
          <video controls autoplay loop muted style="height: 800px; width: auto; object-fit: contain;">
            <source src="static/videos/bench.mp4" type="video/mp4">
          </video>
          <h2 class="subtitle has-text-centered"><i>Three people walk to the bench and sit together.</i></h2>
        </div>
        <div class="item">
          <video controls autoplay loop muted style="height: 800px; width: auto; object-fit: contain;">
            <source src="static/videos/vendingmachine.mp4" type="video/mp4">
          </video>
          <h2 class="subtitle has-text-centered"><i>Two people walk to a vending machine and use it.</i></h2>
        </div>
      </div>
      
    </div>
  </div>
</section>

<!--BibTex citation -->
<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@inproceedings{hhoi,
      title={Learning to Generate Human-Human-Object Interactions from Textual Descriptions}, 
      author={Na, Jeonghyeon and Baik, Sangwon and Lee, Inhee and Lee, Junyoung and Joo, Hanbyul},
      booktitle={NeurIPS},
      year={2025}
}
</code></pre>
  </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a>.
            <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
